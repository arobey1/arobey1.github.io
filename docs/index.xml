<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alex Robey</title>
    <link>https://arobey1.github.io/</link>
    <description>Recent content on Alex Robey</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Sat, 02 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://arobey1.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks</title>
      <link>https://arobey1.github.io/publications/chordal_lip_sdp/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/chordal_lip_sdp/</guid>
      <description>Abstract. Lipschitz constants of neural networks allow for guarantees of robustness in image classification, safety in controller design, and generalizability beyond the training data. As calculating Lipschitz constants is NP-hard, techniques for estimating Lipschitz constants must navigate the trade-off between scalability and accuracy. In this work, we significantly push the scalability frontier of a semidefinite programming technique known as LipSDP while achieving zero accuracy loss. We first show that LipSDP has chordal sparsity, which allows us to derive a chordally sparse formulation that we call Chordal-LipSDP.</description>
    </item>
    
    <item>
      <title>Probabilistically Robust Learning: Balancing Average- and Worst-case Performance</title>
      <link>https://arobey1.github.io/publications/prob_robust_learning/</link>
      <pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/prob_robust_learning/</guid>
      <description>Abstract. Many of the successes of machine learning are based on minimizing an averaged loss function. However, it is well-known that this paradigm suffers from robustness issues that hinder its applicability in safety-critical domains. These issues are often addressed by training against worst-case perturbations of data, a technique known as adversarial training. Although empirically effective, adversarial training can be overly conservative, leading to unfavorable trade-offs between nominal performance and robustness. To this end, in this paper we propose a framework called \emph{probabilistic robustness} that bridges the gap between the accurate, yet brittle average case and the robust, yet conservative worst case by enforcing robustness to most rather than to all perturbations.</description>
    </item>
    
    <item>
      <title>Do deep networks transfer invariances across classes?</title>
      <link>https://arobey1.github.io/publications/do_invariances_transfer/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/do_invariances_transfer/</guid>
      <description>Abstract. To generalize well, classifiers must learn to be invariant to nuisance transformations that do not alter an input&amp;rsquo;s class. Many problems have ``class-agnostic&#39;&#39; nuisance transformations that apply similarly to all classes, such as lighting and background changes for image classification. Neural networks can learn these invariances given sufficient data, but many real-world datasets are heavily class imbalanced and contain only a few examples for most of the classes. We therefore pose the question: how well do neural networks transfer class-agnostic invariances learned from the large classes to the small ones?</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://arobey1.github.io/about/</link>
      <pubDate>Sat, 27 Nov 2021 11:47:44 -0500</pubDate>
      
      <guid>https://arobey1.github.io/about/</guid>
      <description>I&amp;rsquo;m interested in the mathematical and algorithmic foundations of robust learning.
In particular, my recent work has focused on making modern tools from deep learning robust to semantic changes and distributional shifts in data. Over the course of my Ph.D., this topic has allowed me to study ideas from a variety of different fields, including convex optimization, control theory, generative modeling, and high-dimensional statistics.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://arobey1.github.io/cv/</link>
      <pubDate>Sat, 27 Nov 2021 11:47:44 -0500</pubDate>
      
      <guid>https://arobey1.github.io/cv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adversarial Robustness with Semi-Infinite Constrained Learning</title>
      <link>https://arobey1.github.io/publications/semi_inf_robust/</link>
      <pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/semi_inf_robust/</guid>
      <description>Abstract. Despite strong performance in numerous applications, the fragility of deep learning to input perturbations has raised serious questions about its use in safety-critical domains. While adversarial training can mitigate this issue in practice, state-of- the-art methods are increasingly application-dependent, heuristic in nature, and suffer from fundamental trade-offs between nominal performance and robustness. Moreover, the problem of finding worst-case perturbations is non-convex and underparameterized, both of which engender a non-favorable optimization landscape.</description>
    </item>
    
    <item>
      <title>Model-Based Domain Generalization</title>
      <link>https://arobey1.github.io/publications/mbdg/</link>
      <pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/mbdg/</guid>
      <description>Abstract. Despite remarkable success in a variety of applications, it is well-known that deep learning can fail catastrophically when presented with out-of-distribution data. Toward addressing this challenge, we consider the domain generalization problem, wherein predictors are trained using data drawn from a family of related training domains and then evaluated on a distinct and unseen test domain. We show that under a natural model of data generation and a concomitant invariance condition, the domain generalization problem is equivalent to an infinite-dimensional constrained statistical learning problem; this problem forms the basis of our approach, which we call Model-Based Domain Generalization.</description>
    </item>
    
    <item>
      <title>Learning Robust Output Control Barrier Functions from Safe Expert Demonstrations</title>
      <link>https://arobey1.github.io/publications/learning_rocbfs/</link>
      <pubDate>Fri, 22 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/learning_rocbfs/</guid>
      <description>Abstract. This paper addresses learning safe control laws from expert demonstrations. We assume that appropriate models of the system dynamics and the output measurement map are available, along with corresponding error bounds. We first propose robust output control barrier functions (ROCBFs) as a means to guarantee safety, as defined through controlled forward invariance of a safe set. We then present an optimization problem to learn ROCBFs from expert demonstrations that exhibit safe system behavior, e.</description>
    </item>
    
    <item>
      <title>On the Sample Complexity of Stability Constrained Imitation Learning</title>
      <link>https://arobey1.github.io/publications/stab_imitation_learning/</link>
      <pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/stab_imitation_learning/</guid>
      <description>Abstract. We study the following question in the context of imitation learning for continuous control: how are the underlying stability properties of an expert policy reflected in the sample-complexity of an imitation learning task? We provide the first results showing that a surprisingly granular connection can be made between the underlying expert systemâ€™s incremental gain stability, a novel measure of robust convergence between pairs of system trajectories, and the dependency on the task horizon $T$ of the resulting generalization bounds.</description>
    </item>
    
    <item>
      <title>Provable tradeoffs in adversarially robust classification</title>
      <link>https://arobey1.github.io/publications/adv_provable_tradeoffs/</link>
      <pubDate>Thu, 22 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/adv_provable_tradeoffs/</guid>
      <description>Abstract. It is well known that machine learning methods can be vulnerable to adversarially-chosen perturbations of their inputs. Despite significant progress in the area, foundational open problems remain. In this paper, we address several key questions. We derive exact and approximate Bayes-optimal robust classifiers for the important setting of two- and three-class Gaussian classification problems with arbitrary imbalance, for $\ell_2$ and $\ell_\infty$ adversaries. In contrast to classical Bayes-optimal classifiers, determining the optimal decisions here cannot be made pointwise and new theoretical approaches are needed.</description>
    </item>
    
  </channel>
</rss>
