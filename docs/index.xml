<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alex Robey</title>
    <link>https://arobey1.github.io/</link>
    <description>Recent content on Alex Robey</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Thu, 15 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://arobey1.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Probable Domain Generalization via Quantile Risk Minimization</title>
      <link>https://arobey1.github.io/publications/probable_dg/</link>
      <pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/probable_dg/</guid>
      <description>Abstract. Domain generalization (DG) seeks predictors which perform well on unseen test distributions by leveraging labeled training data from multiple related distributions or domains. To achieve this, the standard formulation optimizes for worst-case performance over the set of all possible domains. However, with worst-case shifts very unlikely in practice, this generally leads to overly-conservative solutions. In fact, a recent study found that no DG algorithm outperformed empirical risk minimization in terms of average performance.</description>
    </item>
    
    <item>
      <title>Provable tradeoffs in adversarially robust classification</title>
      <link>https://arobey1.github.io/publications/adv_provable_tradeoffs/</link>
      <pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/adv_provable_tradeoffs/</guid>
      <description>Abstract. It is well known that machine learning methods can be vulnerable to adversarially-chosen perturbations of their inputs. Despite significant progress in the area, foundational open problems remain. In this paper, we address several key questions. We derive exact and approximate Bayes-optimal robust classifiers for the important setting of two- and three-class Gaussian classification problems with arbitrary imbalance, for $\ell_2$ and $\ell_\infty$ adversaries. In contrast to classical Bayes-optimal classifiers, determining the optimal decisions here cannot be made pointwise and new theoretical approaches are needed.</description>
    </item>
    
    <item>
      <title>On the Sample Complexity of Stability Constrained Imitation Learning</title>
      <link>https://arobey1.github.io/publications/stab_imitation_learning/</link>
      <pubDate>Thu, 23 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/stab_imitation_learning/</guid>
      <description>Abstract. We study the following question in the context of imitation learning for continuous control: how are the underlying stability properties of an expert policy reflected in the sample-complexity of an imitation learning task? We provide the first results showing that a surprisingly granular connection can be made between the underlying expert systemâ€™s incremental gain stability, a novel measure of robust convergence between pairs of system trajectories, and the dependency on the task horizon $T$ of the resulting generalization bounds.</description>
    </item>
    
    <item>
      <title>Toward Certified Robustness Against Real-World Distribution Shifts</title>
      <link>https://arobey1.github.io/publications/model_based_verification/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/model_based_verification/</guid>
      <description>Abstract. We consider the problem of certifying the robustness of deep neural networks against real-world distribution shifts. To do so, we bridge the gap between hand-crafted specifications and realistic deployment settings by proposing a novel neural-symbolic verification framework, in which we train a generative model to learn perturbations from data and define specifications with respect to the output of the learned model. A unique challenge arising from this setting is that existing verifiers cannot tightly approximate sigmoid activations, which are fundamental to many state-of-the-art generative models.</description>
    </item>
    
    <item>
      <title>Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks</title>
      <link>https://arobey1.github.io/publications/chordal_lip_sdp/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/chordal_lip_sdp/</guid>
      <description>Abstract. Lipschitz constants of neural networks allow for guarantees of robustness in image classification, safety in controller design, and generalizability beyond the training data. As calculating Lipschitz constants is NP-hard, techniques for estimating Lipschitz constants must navigate the trade-off between scalability and accuracy. In this work, we significantly push the scalability frontier of a semidefinite programming technique known as LipSDP while achieving zero accuracy loss. We first show that LipSDP has chordal sparsity, which allows us to derive a chordally sparse formulation that we call Chordal-LipSDP.</description>
    </item>
    
    <item>
      <title>Probabilistically Robust Learning: Balancing Average- and Worst-case Performance</title>
      <link>https://arobey1.github.io/publications/prob_robust_learning/</link>
      <pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/prob_robust_learning/</guid>
      <description>Abstract. Many of the successes of machine learning are based on minimizing an averaged loss function. However, it is well-known that this paradigm suffers from robustness issues that hinder its applicability in safety-critical domains. These issues are often addressed by training against worst-case perturbations of data, a technique known as adversarial training. Although empirically effective, adversarial training can be overly conservative, leading to unfavorable trade-offs between nominal performance and robustness. To this end, in this paper we propose a framework called \emph{probabilistic robustness} that bridges the gap between the accurate, yet brittle average case and the robust, yet conservative worst case by enforcing robustness to most rather than to all perturbations.</description>
    </item>
    
    <item>
      <title>Do deep networks transfer invariances across classes?</title>
      <link>https://arobey1.github.io/publications/do_invariances_transfer/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/do_invariances_transfer/</guid>
      <description>Abstract. To generalize well, classifiers must learn to be invariant to nuisance transformations that do not alter an input&amp;rsquo;s class. Many problems have ``class-agnostic&#39;&#39; nuisance transformations that apply similarly to all classes, such as lighting and background changes for image classification. Neural networks can learn these invariances given sufficient data, but many real-world datasets are heavily class imbalanced and contain only a few examples for most of the classes. We therefore pose the question: how well do neural networks transfer class-agnostic invariances learned from the large classes to the small ones?</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://arobey1.github.io/about/</link>
      <pubDate>Sat, 27 Nov 2021 11:47:44 -0500</pubDate>
      
      <guid>https://arobey1.github.io/about/</guid>
      <description>I&amp;rsquo;m interested in the mathematical and algorithmic foundations of robust learning.
In particular, my recent work has focused on making modern tools from deep learning robust to semantic changes and distributional shifts in data. Over the course of my Ph.D., this topic has allowed me to study ideas from a variety of different fields, including convex optimization, control theory, generative modeling, and high-dimensional statistics.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://arobey1.github.io/cv/</link>
      <pubDate>Sat, 27 Nov 2021 11:47:44 -0500</pubDate>
      
      <guid>https://arobey1.github.io/cv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adversarial Robustness with Semi-Infinite Constrained Learning</title>
      <link>https://arobey1.github.io/publications/semi_inf_robust/</link>
      <pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/semi_inf_robust/</guid>
      <description>Abstract. Despite strong performance in numerous applications, the fragility of deep learning to input perturbations has raised serious questions about its use in safety-critical domains. While adversarial training can mitigate this issue in practice, state-of- the-art methods are increasingly application-dependent, heuristic in nature, and suffer from fundamental trade-offs between nominal performance and robustness. Moreover, the problem of finding worst-case perturbations is non-convex and underparameterized, both of which engender a non-favorable optimization landscape.</description>
    </item>
    
  </channel>
</rss>
