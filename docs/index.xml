<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alex Robey</title>
    <link>https://arobey1.github.io/</link>
    <description>Recent content on Alex Robey</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Wed, 19 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://arobey1.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Adversarial Training Should Be Cast As a Non-Zero-Sum Game</title>
      <link>https://arobey1.github.io/publications/adv_non_zero_sum/</link>
      <pubDate>Wed, 19 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/adv_non_zero_sum/</guid>
      <description>Abstract. One prominent approach toward resolving the adversarial vulnerability of deep neural networks is the two-player zero-sum paradigm of adversarial training, in which predictors are trained against adversarially-chosen perturbations of data. Despite the promise of this approach, algorithms based on this paradigm have not engendered sufficient levels of robustness, and suffer from pathological behavior like robust overfitting. To understand this shortcoming, we first show that the commonly used surrogate-based relaxation used in adversarial training algorithms voids all guarantees on the robustness of trained classifiers.</description>
    </item>
    
    <item>
      <title>Toward Certified Robustness Against Real-World Distribution Shifts</title>
      <link>https://arobey1.github.io/publications/model_based_verification/</link>
      <pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/model_based_verification/</guid>
      <description>Abstract. We consider the problem of certifying the robustness of deep neural networks against real-world distribution shifts. To do so, we bridge the gap between hand-crafted specifications and realistic deployment settings by proposing a novel neural-symbolic verification framework, in which we train a generative model to learn perturbations from data and define specifications with respect to the output of the learned model. A unique challenge arising from this setting is that existing verifiers cannot tightly approximate sigmoid activations, which are fundamental to many state-of-the-art generative models.</description>
    </item>
    
    <item>
      <title>Loops</title>
      <link>https://arobey1.github.io/python/loops/</link>
      <pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/python/loops/</guid>
      <description>We&amp;rsquo;ve been building to this day for quite some time. Today is the day we learn about loops! If you haven&amp;rsquo;t heard about loops before, you&amp;rsquo;re in for a real treat; loops are an essential part of almost every programming language.
 What is a loop? Loops are ubiquitous in Python. Once you master loops, you&amp;rsquo;ll be amazed at how much more your programs can do. But first things first: What is a loop?</description>
    </item>
    
    <item>
      <title>Conditionals</title>
      <link>https://arobey1.github.io/python/conditionals/</link>
      <pubDate>Wed, 04 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/python/conditionals/</guid>
      <description>Today is a big day! We&amp;rsquo;re going to meet one of the most essential parts of the Python language: conditionals. After you&amp;rsquo;ve mastered conditionals, you&amp;rsquo;ll be amazed at how much more you can do in Python. Let&amp;rsquo;s get to work!
 What is programming and why do we do it? These are big questions. There are many reasons why we write programs. Perhaps you&amp;rsquo;re a data scientist who wants to use sales data to decide whether or not to sell a particular product.</description>
    </item>
    
    <item>
      <title>Lists</title>
      <link>https://arobey1.github.io/python/lists/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/python/lists/</guid>
      <description>You&amp;rsquo;re back! We&amp;rsquo;ve got a lot to cover in this lesson, so let&amp;rsquo;s get right into it. The goal is introduce another fundamental data type: the list. Lists are crucial to almost every programming language out there, and Python is no excetpion.
 A motivating example: Why we need lists in Python Let&amp;rsquo;s say that you&amp;rsquo;re an avid reader. You read all sorts of things. Technical books, novels, the occasional murder mystery, biographies, poetry, you name it!</description>
    </item>
    
    <item>
      <title>Math operations</title>
      <link>https://arobey1.github.io/python/math_operations/</link>
      <pubDate>Sat, 24 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/python/math_operations/</guid>
      <description>Welcome back! I hope you learned something new in the previous lesson on variables. Today, we are going to shift our focus to math. And if that makes you a bit nervous, don&amp;rsquo;t worry! We won&amp;rsquo;t be doing anything complicated today. You&amp;rsquo;ll just need the basics: addition, subtraction, exponents, and the like.
 The basic operations Since we&amp;rsquo;re talking about math in this lesson, we&amp;rsquo;re largely going to focus on numerical data types; that is, ints and floats.</description>
    </item>
    
    <item>
      <title>Variables</title>
      <link>https://arobey1.github.io/python/variables/</link>
      <pubDate>Wed, 21 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/python/variables/</guid>
      <description>Variables and data types Perhaps the most fundamental of all Python fundamentals is the concept of a variable. The first thing you need to know about variables is what they&amp;rsquo;re used for:
 A variable is a container for saving data.
 A theme in this set of notes will be illustrating different concepts by way of examples. And I can think of no better way to introduce variables than with the following (not-so-randomly-chosen) use case.</description>
    </item>
    
    <item>
      <title>Interacting with Python</title>
      <link>https://arobey1.github.io/python/interacting_with_python/</link>
      <pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/python/interacting_with_python/</guid>
      <description>Interacting with Python There are two main ways of interacting with Python: with a shell and with a script. Here&amp;rsquo;s a brief primer on how shells and scripts work and when you should use them.
 Python shell Perhaps the easiest way to start interacting with Python is through what&amp;rsquo;s known as a shell. Never heard of a shell before? No worries! Here&amp;rsquo;s a quick definition:
 A shell is an interpreter that can execute Python programs and simple Python commands.</description>
    </item>
    
    <item>
      <title>Introduction to Python</title>
      <link>https://arobey1.github.io/python/intro/</link>
      <pubDate>Sat, 17 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/python/intro/</guid>
      <description>Psst! Hey you. Yes you! You look like you&amp;rsquo;re ready to learn Python. And yes, you&amp;rsquo;re right, I can&amp;rsquo;t see you. But I know you wouldn&amp;rsquo;t be here if you weren&amp;rsquo;t ready to go. So let&amp;rsquo;s do this!
 What is Python? Python is a programming language that has become ubiquitous in fields such as data science and machine learning. The syntax is relatively simple, and with the right guidance, it&amp;rsquo;s easy to get started!</description>
    </item>
    
    <item>
      <title>Probable Domain Generalization via Quantile Risk Minimization</title>
      <link>https://arobey1.github.io/publications/probable_dg/</link>
      <pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/probable_dg/</guid>
      <description>Abstract. Domain generalization (DG) seeks predictors which perform well on unseen test distributions by leveraging labeled training data from multiple related distributions or domains. To achieve this, the standard formulation optimizes for worst-case performance over the set of all possible domains. However, with worst-case shifts very unlikely in practice, this generally leads to overly-conservative solutions. In fact, a recent study found that no DG algorithm outperformed empirical risk minimization in terms of average performance.</description>
    </item>
    
  </channel>
</rss>
