<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alex Robey</title>
    <link>https://arobey1.github.io/</link>
    <description>Recent content on Alex Robey</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Sat, 24 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://arobey1.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Math operations</title>
      <link>https://arobey1.github.io/python/math_operations/</link>
      <pubDate>Sat, 24 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/python/math_operations/</guid>
      <description>Welcome back! I hope you learned something new in the previous lesson on variables, data types, and comparison. Today, we are going to shift our focus to math. And if that makes you a bit nervous, don&amp;rsquo;t worry! We won&amp;rsquo;t be doing anything complicated today. You&amp;rsquo;ll just need the basics: addition, subtraction, exponents, and the like.
 The basic operations Since we&amp;rsquo;re talking about math in this lesson, we&amp;rsquo;re largely going to focus on numerical data types; that is, ints and floats.</description>
    </item>
    
    <item>
      <title>Variables, data types, and comparison</title>
      <link>https://arobey1.github.io/python/variables_data_types/</link>
      <pubDate>Wed, 21 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/python/variables_data_types/</guid>
      <description>Variables and data types Perhaps the most fundamental of all Python fundamentals is the concept of a variable. The first thing you need to know about variables is what they&amp;rsquo;re used for:
 A variable is a container for saving data.
 A theme in this set of notes will be illustrating different concepts by way of examples. And I can think of no better way to introduce variables than with the following (not-so-randomly-chosen) use case.</description>
    </item>
    
    <item>
      <title>Interacting with Python</title>
      <link>https://arobey1.github.io/python/interacting_with_python/</link>
      <pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/python/interacting_with_python/</guid>
      <description>Interacting with Python There are two main ways of interacting with a Python: with a shell and with a script. Here&amp;rsquo;s a brief primer on how shells and scripts work and when you should use them.
Python shell Perhaps the easiest way to start interacting with Python is through what&amp;rsquo;s known as a shell. Never heard of a shell before? No worries! Here&amp;rsquo;s a quick definition:
 A shell is an interpreter that can execute Python programs and simple Python commands.</description>
    </item>
    
    <item>
      <title>Introduction to Python</title>
      <link>https://arobey1.github.io/python/intro/</link>
      <pubDate>Sat, 17 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/python/intro/</guid>
      <description>Psst! Hey you. Yes you! You look like you&amp;rsquo;re ready to learn Python. And yes, you&amp;rsquo;re right, I can&amp;rsquo;t see you. But I know you wouldn&amp;rsquo;t be here if you weren&amp;rsquo;t ready to go. So let&amp;rsquo;s do this!
 What is Python? Python is a programming language that has become ubiquitous in fields such as data science and machine learning. The syntax is relatively simple, and with the right guidance, it&amp;rsquo;s easy to get started!</description>
    </item>
    
    <item>
      <title>Toward Certified Robustness Against Real-World Distribution Shifts</title>
      <link>https://arobey1.github.io/publications/model_based_verification/</link>
      <pubDate>Fri, 14 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/model_based_verification/</guid>
      <description>Abstract. We consider the problem of certifying the robustness of deep neural networks against real-world distribution shifts. To do so, we bridge the gap between hand-crafted specifications and realistic deployment settings by proposing a novel neural-symbolic verification framework, in which we train a generative model to learn perturbations from data and define specifications with respect to the output of the learned model. A unique challenge arising from this setting is that existing verifiers cannot tightly approximate sigmoid activations, which are fundamental to many state-of-the-art generative models.</description>
    </item>
    
    <item>
      <title>Probable Domain Generalization via Quantile Risk Minimization</title>
      <link>https://arobey1.github.io/publications/probable_dg/</link>
      <pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/probable_dg/</guid>
      <description>Abstract. Domain generalization (DG) seeks predictors which perform well on unseen test distributions by leveraging labeled training data from multiple related distributions or domains. To achieve this, the standard formulation optimizes for worst-case performance over the set of all possible domains. However, with worst-case shifts very unlikely in practice, this generally leads to overly-conservative solutions. In fact, a recent study found that no DG algorithm outperformed empirical risk minimization in terms of average performance.</description>
    </item>
    
    <item>
      <title>Provable tradeoffs in adversarially robust classification</title>
      <link>https://arobey1.github.io/publications/adv_provable_tradeoffs/</link>
      <pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/adv_provable_tradeoffs/</guid>
      <description>Abstract. It is well known that machine learning methods can be vulnerable to adversarially-chosen perturbations of their inputs. Despite significant progress in the area, foundational open problems remain. In this paper, we address several key questions. We derive exact and approximate Bayes-optimal robust classifiers for the important setting of two- and three-class Gaussian classification problems with arbitrary imbalance, for $\ell_2$ and $\ell_\infty$ adversaries. In contrast to classical Bayes-optimal classifiers, determining the optimal decisions here cannot be made pointwise and new theoretical approaches are needed.</description>
    </item>
    
    <item>
      <title>On the Sample Complexity of Stability Constrained Imitation Learning</title>
      <link>https://arobey1.github.io/publications/stab_imitation_learning/</link>
      <pubDate>Thu, 23 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/stab_imitation_learning/</guid>
      <description>Abstract. We study the following question in the context of imitation learning for continuous control: how are the underlying stability properties of an expert policy reflected in the sample-complexity of an imitation learning task? We provide the first results showing that a surprisingly granular connection can be made between the underlying expert systemâ€™s incremental gain stability, a novel measure of robust convergence between pairs of system trajectories, and the dependency on the task horizon $T$ of the resulting generalization bounds.</description>
    </item>
    
    <item>
      <title>Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks</title>
      <link>https://arobey1.github.io/publications/chordal_lip_sdp/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/chordal_lip_sdp/</guid>
      <description>Abstract. Lipschitz constants of neural networks allow for guarantees of robustness in image classification, safety in controller design, and generalizability beyond the training data. As calculating Lipschitz constants is NP-hard, techniques for estimating Lipschitz constants must navigate the trade-off between scalability and accuracy. In this work, we significantly push the scalability frontier of a semidefinite programming technique known as LipSDP while achieving zero accuracy loss. We first show that LipSDP has chordal sparsity, which allows us to derive a chordally sparse formulation that we call Chordal-LipSDP.</description>
    </item>
    
    <item>
      <title>Probabilistically Robust Learning: Balancing Average- and Worst-case Performance</title>
      <link>https://arobey1.github.io/publications/prob_robust_learning/</link>
      <pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://arobey1.github.io/publications/prob_robust_learning/</guid>
      <description>Abstract. Many of the successes of machine learning are based on minimizing an averaged loss function. However, it is well-known that this paradigm suffers from robustness issues that hinder its applicability in safety-critical domains. These issues are often addressed by training against worst-case perturbations of data, a technique known as adversarial training. Although empirically effective, adversarial training can be overly conservative, leading to unfavorable trade-offs between nominal performance and robustness. To this end, in this paper we propose a framework called \emph{probabilistic robustness} that bridges the gap between the accurate, yet brittle average case and the robust, yet conservative worst case by enforcing robustness to most rather than to all perturbations.</description>
    </item>
    
  </channel>
</rss>
