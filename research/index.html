<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="../img/icons/soccerball.png">

    <title>Alex Robey :: Research</title>
    <link rel="stylesheet" href="../styles.css">
    <script src="../navbar/navbar-loader.js"></script>
</head>
<body>
    <div class="container">
        <!-- The navbar will be loaded here by JavaScript -->
        <header></header>
        
        <main>


            <div class="focus-area-section" id="focus-area-section">

                <h2 class="section-heading"><span>Focus Areas</span></h2>

                <div class="focus-area-item">
                    <h4 class="focus-subtitle">1. AI safety</h3>
                    <p>
                        Making text generation models like OpenAI's ChatGPT safe for humans to use is a problem that cannot be solved by algorithms alone. A collective effort to adjust the governance of AI, design content filters, continuously monitor and probe the vulnerabilites is needed. 
                    </p>

                    <figure class="research-image-with-caption">
                        <img src="../img/research/aisafety.png" id="aisafety-image" alt="Nominal LLM GIF" class="wide-image">
                        <figcaption><span class="figure-label"></span>Large language models, like OpenAI's ChatGPT, are vulnerable to attacks that cause these models to generate harmful content.</figcaption>
                    </figure>

                    <p>
                        The technical part of my research agenda involves designing <a href="https://arxiv.org/abs/2310.08419" target="_blank">attacks</a>, <a href="https://arxiv.org/abs/2310.03684" target="_blank">defenses</a>, and <a href="https://arxiv.org/pdf/2404.01318">benchmarks</a> to stress test large models that process text, images, and speech. For example, I proposed SmoothLLM, one of the first defenses against jailbreaking attacks.
                    </p>

                    <figure class="research-image-with-caption">
                        <img src="../img/research/smoothllm.png" id="smoothllm-image" alt="Nominal LLM GIF" class="wide-image">
                        <figcaption><span class="figure-label"></span>SmoothLLM defends large language models against adversarial attacks.</figcaption>
                    </figure>

                    <p>
                        I'm also interested in (1) understanding the mechanisms and data that cause large models to generate harmful content and (2) measuring the vulnerabilities of large models when used in fields like robotics.
                    </p>


                    <figure class="research-image-with-caption">
                        <img src="../img/writing/jailbreaking-robots/behaviors.png" id="robot-jailbreaking" alt="Nominal LLM GIF" class="wide-image">
                        <figcaption><span class="figure-label"></span>We show that LLM-controlled robots can be jailbroken with 100% success rates.</figcaption>
                    </figure>

                    <p>
                        Outside of academia, I'm interested in contributing to the ongoing debate about how AI models should be goverened. I was recently part of a <a href="https://arxiv.org/abs/2403.04893" target="_blank">public policy proposal</a> and <a href="https://sites.mit.edu/ai-safe-harbor/" target="_blank"> open letter</a>, which were later covered in <a href="https://www.washingtonpost.com/technology/2024/03/05/ai-research-letter-openai-meta-midjourney/" target="_blank">The Washington Post</a>, calling for more robust oversight of large models.
                    </p>
                </div>
                
                <div class="focus-area-item">
                    <h4 class="focus-subtitle">2. Out-of-distribution generalization</h3>
                    <p>
                        Deep learning has an amazing capacity to recognize and interpret the data it sees during training. But what happens when neural networks interacts with data very different from what they've seen before?
                    </p>

                    <figure class="research-image-with-caption">
                        <img src="../img/research/mbdg.png" alt="Nominal LLM GIF" class="wide-image">
                        <figcaption><span class="figure-label"></span>An overview of out-of-distribution generalization in medical imaging.</figcaption>
                    </figure>

                    <p>
                        This problem is called out-of-distribution (OOD) generalization. My work in this area, which uses tools from robust optimization theory and generative models, has looked at OOD problems in <a href="https://arxiv.org/pdf/2005.10247" target="_blank">self-driving</a>, <a href="https://proceedings.neurips.cc/paper/2021/file/a8f12d9486cbcc2fe0cfc5352011ad35-Paper.pdf" target="_blank">medical imaging</a>, and <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/6f11132f6ecbbcafafdf6decfc98f7be-Paper-Conference.pdf" target="_blank">drug discovery</a>. I am also interested in algorithms that yield <a href="https://ieeexplore.ieee.org/document/10136136" target="_blank"><em>provable</em> guarantees</a> on the performance of models when evaluated OOD.
                    </p>
                </div>

                <div class="focus-area-item">
                    <h4 class="focus-subtitle">3. Adversarial Robustness</h3>
                    <p>
                        Much has been written about the the tendency of neural networks to make incorrect predictions when their input data is perturbed by a malicious, or even adversarial, user. Despite <a href="https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html">thousands of papers on the topic</a>, it remains unclear how to make neural networks more robust. 
                    </p>

                    <figure class="research-image-with-caption">
                        <img src="../img/research/dists.png" alt="Nominal LLM GIF" class="wide-image">
                        <figcaption><span class="figure-label"></span>View of adversarial robustness from the dual perspective.</figcaption>
                    </figure>

                    <p>
                        My work on robustness is guided my two mantras:
                    </p>
                    <ul class="focus-list">
                        <li>Designing robust defences requires first identifying strong attacks.</li>
                        <li>Vulnerabilites should be identified, open-sourced, and resolved as fast as possible, but no faster.</li>
                    </ul>
                    <p>
                        I'm interested in designing new attacks and defenses for neural networks in the setting of perturbation-based, norm-bounded adversaries, and in understanding the fundamental, statistical limits of how robust different architectures can be. 
                    </p>
                    <figure class="research-image-with-caption">
                        <img src="../img/research/prob-rob.png" alt="Nominal LLM GIF" class="wide-image" id="prob-rob">
                        <figcaption><span class="figure-label"></span>Interpolation between average- and worst-case robustness.</figcaption>
                    </figure>
                    <p>
                        My research involves <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/312ecfdfa8b239e076b114498ce21905-Paper.pdf" target="_blank">duality-inspired defense algorithms</a> and <a href="https://proceedings.mlr.press/v162/robey22a/robey22a.pdf" target="_blank">probabilisitc perspectives</a> that interpolates between average- and worst-case robustness.
                    </p>
                </div>

            </div>

        
</body>
</html>
