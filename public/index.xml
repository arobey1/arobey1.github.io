<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alex Robey</title>
    <link>https://example.com/</link>
    <description>Recent content on Alex Robey</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Fri, 26 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://example.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About</title>
      <link>https://example.com/about/</link>
      <pubDate>Sat, 27 Nov 2021 11:47:44 -0500</pubDate>
      
      <guid>https://example.com/about/</guid>
      <description>I&amp;rsquo;m interested in the mathematical and algorithmic foundations of robust learning.
In particular, my recent work has focused on making modern tools from deep learning robust to semantic changes and distributional shifts in data. Over the course of my Ph.D., this topic has allowed me to study ideas from a wide variety of different fields, including convex optimization, control theory, generative modeling, and high-dimensional statistics.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://example.com/cv/</link>
      <pubDate>Sat, 27 Nov 2021 11:47:44 -0500</pubDate>
      
      <guid>https://example.com/cv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adversarial Robustness with Semi-Infinite Constrained Learning</title>
      <link>https://example.com/publications/semi_inf_robust/</link>
      <pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/publications/semi_inf_robust/</guid>
      <description>Abstract. Despite strong performance in numerous applications, the fragility of deep learning to input perturbations has raised serious questions about its use in safety-critical domains. While adversarial training can mitigate this issue in practice, state-of- the-art methods are increasingly application-dependent, heuristic in nature, and suffer from fundamental trade-offs between nominal performance and robustness. Moreover, the problem of finding worst-case perturbations is non-convex and underparameterized, both of which engender a non-favorable optimization landscape.</description>
    </item>
    
    <item>
      <title>Model-Based Domain Generalization</title>
      <link>https://example.com/publications/mbdg/</link>
      <pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/publications/mbdg/</guid>
      <description>Abstract. Despite remarkable success in a variety of applications, it is well-known that deep learning can fail catastrophically when presented with out-of-distribution data. Toward addressing this challenge, we consider the domain generalization problem, wherein predictors are trained using data drawn from a family of related training domains and then evaluated on a distinct and unseen test domain. We show that under a natural model of data generation and a concomitant invariance condition, the domain generalization problem is equivalent to an infinite-dimensional constrained statistical learning problem; this problem forms the basis of our approach, which we call Model-Based Domain Generalization.</description>
    </item>
    
    <item>
      <title>Learning Robust Output Control Barrier Functions from Safe Expert Demonstrations</title>
      <link>https://example.com/publications/learning_rocbfs/</link>
      <pubDate>Fri, 22 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/publications/learning_rocbfs/</guid>
      <description>Abstract. This paper addresses learning safe control laws from expert demonstrations. We assume that appropriate models of the system dynamics and the output measurement map are available, along with corresponding error bounds. We first propose robust output control barrier functions (ROCBFs) as a means to guarantee safety, as defined through controlled forward invariance of a safe set. We then present an optimization problem to learn ROCBFs from expert demonstrations that exhibit safe system behavior, e.</description>
    </item>
    
    <item>
      <title>On the Sample Complexity of Stability Constrained Imitation Learning</title>
      <link>https://example.com/publications/stab_imitation_learning/</link>
      <pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/publications/stab_imitation_learning/</guid>
      <description>Abstract. We study the following question in the context of imitation learning for continuous control: how are the underlying stability properties of an expert policy reflected in the sample-complexity of an imitation learning task? We provide the first results showing that a surprisingly granular connection can be made between the underlying expert system’s incremental gain stability, a novel measure of robust convergence between pairs of system trajectories, and the dependency on the task horizon $T$ of the resulting generalization bounds.</description>
    </item>
    
    <item>
      <title>Provable tradeoffs in adversarially robust classification</title>
      <link>https://example.com/publications/adv_provable_tradeoffs/</link>
      <pubDate>Thu, 22 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/publications/adv_provable_tradeoffs/</guid>
      <description>Abstract. It is well known that machine learning methods can be vulnerable to adversarially-chosen perturbations of their inputs. Despite significant progress in the area, foundational open problems remain. In this paper, we address several key questions. We derive exact and approximate Bayes-optimal robust classifiers for the important setting of two- and three-class Gaussian classification problems with arbitrary imbalance, for $\ell_2$ and $\ell_\infty$ adversaries. In contrast to classical Bayes-optimal classifiers, determining the optimal decisions here cannot be made pointwise and new theoretical approaches are needed.</description>
    </item>
    
    <item>
      <title>Learning Robust Hybrid Control Barrier Functions for Uncertain Systems</title>
      <link>https://example.com/publications/learning_rhcbfs/</link>
      <pubDate>Wed, 07 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/publications/learning_rhcbfs/</guid>
      <description>Abstract. The need for robust control laws is especially important in safety-critical applications. We propose robust hybrid control barrier functions as a means to synthesize control laws that ensure robust safety. Based on this notion, we formulate an optimization problem for learning robust hybrid control barrier functions from data. We identify sufficient conditions on the data such that feasibility of the optimization problem ensures correctness of the learned robust hybrid control barrier functions.</description>
    </item>
    
    <item>
      <title>Optimal Algorithms for Submodular Maximization with Distributed Constraints</title>
      <link>https://example.com/publications/submodular/</link>
      <pubDate>Mon, 07 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/publications/submodular/</guid>
      <description>Abstract. We consider a class of discrete optimization problems that aim to maximize a submodular objective function subject to a distributed partition matroid constraint. More precisely, we consider a networked scenario in which multiple agents choose actions from local strategy sets with the goal of maximizing a submodular objective function defined over the set of all possible actions. Given this distributed setting, we develop Constraint-Distributed Continuous Greedy (CDCG), a message passing algorithm that converges to the tight $(1 − \frac{1}{e})$ approximation factor of the optimum global solution using only local computation and communication.</description>
    </item>
    
    <item>
      <title>Learning control barrier functions from expert demonstrations</title>
      <link>https://example.com/publications/learning_cbfs/</link>
      <pubDate>Mon, 14 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/publications/learning_cbfs/</guid>
      <description>Abstract. Inspired by the success of imitation and inverse reinforcement learning in replicating expert behavior through optimal control, we propose a learning based approach to safe controller synthesis based on control barrier functions (CBFs). We consider the setting of a known nonlinear control affine dynamical system and assume that we have access to safe trajectories generated by an expert - a practical example of such a setting would be a kinematic model of a self-driving vehicle with safe trajectories (e.</description>
    </item>
    
  </channel>
</rss>
